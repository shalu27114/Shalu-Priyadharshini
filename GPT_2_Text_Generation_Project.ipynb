{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalu27114/Shalu-Priyadharshini/blob/main/GPT_2_Text_Generation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zJ4iMQmok24",
        "outputId": "efed984a-a52d-4450-ace1-0ad7cad8016b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Google Colab Ready!\n",
            "‚úÖ PyTorch: 2.9.0+cpu\n",
            "‚úÖ GPU Available: False\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# COMPLETE GPT-2 TEXT GENERATION PROJECT\n",
        "# =============================================\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from google.colab import files\n",
        "import time\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Google Colab Ready!\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Fc-ovnp4am",
        "outputId": "04e9a39f-a695-4df8-bf7a-bb9f878cf715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Loading GPT-2 model...\n",
            "This downloads ~500MB (first time only)\n",
            "Please wait 2-3 minutes...\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded in 0.62 seconds!\n",
            "\n",
            "ü§ñ Quick test...\n",
            "Test output: Hello AI!\n",
            "\n",
            "I think our AI has a pretty good idea of what it wants. Our AI is kind of a little different. It doesn't want anything too big or too small. So it's just like anything else.\n",
            "\n",
            "What are your thoughts on this situation? What are your thoughts on these two characters? What are your thoughts on the way the AI has responded to the situation? What do you think are the most important things for the AI?\n",
            "\n",
            "We're very excited about this. We've been working on this for a long time. We've been working on it for a long time. We have a lot of people working on it, but we've still got some things to do.\n",
            "\n",
            "Do you have some ideas or ideas for the AI, or are you trying to write a game where you have to guess how you're going to play?\n",
            "\n",
            "We're trying to write a game where you can think about what you want to do. We're trying to write a game where you can choose how you want to play, and then you just have to be creative.\n",
            "\n",
            "We're playing with a whole bunch of different options. One is to play a game with a lot of different options. Like if you can play a game where you\n",
            "==================================================\n",
            "‚úÖ READY! Run the next cell for interactive mode.\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Test GPT-2 with progress indicator\n",
        "print(\"üöÄ Loading GPT-2 model...\")\n",
        "print(\"This downloads ~500MB (first time only)\")\n",
        "print(\"Please wait 2-3 minutes...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "# Show loading progress\n",
        "start_time = time.time()\n",
        "\n",
        "# Load with progress\n",
        "text_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚úÖ Model loaded in {end_time - start_time:.2f} seconds!\")\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nü§ñ Quick test...\")\n",
        "test_result = text_generator(\"Hello AI!\", max_length=30, num_return_sequences=1)\n",
        "print(f\"Test output: {test_result[0]['generated_text']}\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ READY! Run the next cell for interactive mode.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8581guQqwX7",
        "outputId": "5d238bb8-7a69-4c8c-d940-57530df5faf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ SIMPLIFIED GPT-2 TEXT GENERATOR\n",
            "==================================================\n",
            "Type prompts and press Enter. Type 'quit' to exit.\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Choose a prompt number (1-10) or type your own:\n",
            "1. Artificial intelligence is\n",
            "2. The future of technology will be\n",
            "3. Once upon a time in Silicon Valley,\n",
            "4. Machine learning can help us\n",
            "5. In the year 2050,\n",
            "6. Love is like\n",
            "7. The secret to happiness is\n",
            "8. If I could travel back in time,\n",
            "9. The best pizza topping is\n",
            "10. Why do we dream about\n",
            "\n",
            "==================================================\n",
            "\n",
            "Enter number (1-10) or type your prompt: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: 'The future of technology will be'\n",
            "\n",
            "‚è≥ Generating from: 'The future of technology will be'...\n",
            "\n",
            "============================================================\n",
            "üìù GENERATED TEXT:\n",
            "============================================================\n",
            "The future of technology will be changed by our technology. But\n",
            "that's not the world we live in. We're still using our brains. We\n",
            "still call our kids 'kids' when they grow up, when they get older\n",
            "(or, as the case may be, at the age of 4). We've got new\n",
            "technologies that you can use for years, decades, if you want to.\n",
            "We have new ways of delivering information faster and simpler.\n",
            "We've got new technologies that people are using, that they can\n",
            "spend money for. We have new ways of doing things. We have new\n",
            "technologies that help us make a change, or build a better world.\n",
            "We're not going to go through the motions of using all of those\n",
            "technologies. We're not going to change the world like we did\n",
            "when we built the internet. That's the wrong time frame to change\n",
            "our world.  For all the latest Opinion News, download Indian\n",
            "Express App\n",
            "============================================================\n",
            "\n",
            "‚ú® CONTINUATION:\n",
            "----------------------------------------\n",
            "changed by our technology. But that's not the world we live in.\n",
            "We're still using our brains. We still call our kids 'kids' when\n",
            "they grow up, when they get older (or, as the case may be, at the\n",
            "age of 4). We've got new technologies that you can use for years,\n",
            "decades, if you want to. We have new ways of delivering\n",
            "information faster and simpler. We've got new technologies that\n",
            "people are using, that they can spend money for. We have new ways\n",
            "of doing things. We have new technologies that help us make a\n",
            "change, or build a better world. We're not going to go through\n",
            "the motions of using all of those technologies. We're not going\n",
            "to change the world like we did when we built the internet.\n",
            "That's the wrong time frame to change our world.  For all the\n",
            "latest Opinion News, download Indian Express App\n",
            "\n",
            "==================================================\n",
            "\n",
            "Enter number (1-10) or type your prompt: quit\n",
            "\n",
            "üëã Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# CELL 3: SIMPLIFIED Interactive Generator (No waiting)\n",
        "print(\"ü§ñ SIMPLIFIED GPT-2 TEXT GENERATOR\")\n",
        "print(\"=\"*50)\n",
        "print(\"Type prompts and press Enter. Type 'quit' to exit.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Use the already loaded generator\n",
        "generator = text_generator\n",
        "\n",
        "# Pre-defined prompts to choose from (no waiting)\n",
        "sample_prompts = [\n",
        "    \"Artificial intelligence is\",\n",
        "    \"The future of technology will be\",\n",
        "    \"Once upon a time in Silicon Valley,\",\n",
        "    \"Machine learning can help us\",\n",
        "    \"In the year 2050,\",\n",
        "    \"Love is like\",\n",
        "    \"The secret to happiness is\",\n",
        "    \"If I could travel back in time,\",\n",
        "    \"The best pizza topping is\",\n",
        "    \"Why do we dream about\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìù Choose a prompt number (1-10) or type your own:\")\n",
        "for i, prompt in enumerate(sample_prompts, 1):\n",
        "    print(f\"{i}. {prompt}\")\n",
        "\n",
        "while True:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    choice = input(\"\\nEnter number (1-10) or type your prompt: \").strip()\n",
        "\n",
        "    if choice.lower() == 'quit':\n",
        "        print(\"\\nüëã Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Check if it's a number choice\n",
        "    if choice.isdigit() and 1 <= int(choice) <= 10:\n",
        "        prompt = sample_prompts[int(choice) - 1]\n",
        "        print(f\"Using: '{prompt}'\")\n",
        "    else:\n",
        "        prompt = choice\n",
        "\n",
        "    if not prompt:\n",
        "        print(\"Please enter something!\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n‚è≥ Generating from: '{prompt}'...\")\n",
        "\n",
        "    # Generate with reasonable length\n",
        "    try:\n",
        "        result = generator(\n",
        "            prompt,\n",
        "            max_length=100,  # Fixed length\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.8,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        generated = result[0]['generated_text']\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìù GENERATED TEXT:\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Format nicely\n",
        "        import textwrap\n",
        "        lines = textwrap.wrap(generated, width=65)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Show continuation\n",
        "        if len(generated) > len(prompt):\n",
        "            continuation = generated[len(prompt):].strip()\n",
        "            if continuation:\n",
        "                print(\"\\n‚ú® CONTINUATION:\")\n",
        "                print(\"-\"*40)\n",
        "                cont_lines = textwrap.wrap(continuation, width=65)\n",
        "                for line in cont_lines:\n",
        "                    print(line)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        print(\"The model might still be loading...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSyznQfgrCAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78dbdbf0-fe47-4789-e001-22eb843c2328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä PREPARING CUSTOM DATASET\n",
            "==================================================\n",
            "Sample dataset created!\n",
            "\n",
            "üìù Dataset preview:\n",
            "----------------------------------------\n",
            " Artificial intelligence is revolutionizing technology. AI systems can\n",
            "now understand and generate human language. Machine learning\n",
            "algorithms analyze patterns in data to make predictions. Deep learning\n",
            "models like GPT-2 create coherent text. Natural language processing\n",
            "enables computers to read and...\n",
            "----------------------------------------\n",
            "\n",
            "‚úÖ Dataset saved as 'custom_dataset.txt'\n"
          ]
        }
      ],
      "source": [
        "# CELL 4: Prepare Custom Dataset\n",
        "print(\"üìä PREPARING CUSTOM DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Sample dataset (you can replace this with your own)\n",
        "custom_data = \"\"\"\n",
        "Artificial intelligence is revolutionizing technology. AI systems can now understand and generate human language.\n",
        "Machine learning algorithms analyze patterns in data to make predictions. Deep learning models like GPT-2 create coherent text.\n",
        "Natural language processing enables computers to read and write. Text generation has applications in chatbots and content creation.\n",
        "The future of AI includes more sophisticated language models. These models will transform how we interact with computers.\n",
        "Creative writing with AI is becoming more accessible. Authors can use AI as a tool for inspiration and drafting.\n",
        "Technology continues to advance at a rapid pace. Innovation in AI will shape the coming decades.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sample dataset created!\")\n",
        "print(\"\\nüìù Dataset preview:\")\n",
        "print(\"-\"*40)\n",
        "print(textwrap.fill(custom_data[:300] + \"...\", width=70))\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Save dataset\n",
        "with open('custom_dataset.txt', 'w') as f:\n",
        "    f.write(custom_data)\n",
        "print(\"\\n‚úÖ Dataset saved as 'custom_dataset.txt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUV7mn44rcxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69869416-ed9c-4571-d6af-27b90909dba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "‚úçÔ∏è  CREATIVE WRITING ASSISTANT\n",
            "============================================================\n",
            "\n",
            "Choose a writing style:\n",
            "1. Story/Tale\n",
            "2. Technical/Article\n",
            "3. Poetic/Creative\n",
            "4. Business/Professional\n",
            "5. Custom style\n",
            "\n",
            "Choose style (1-5): 2\n",
            "\n",
            "Style: Explain technically how\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Enter your topic (or 'quit'): how computer works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÆ Generating: 'Explain technically howhow computer works'\n",
            "\n",
            "üí° Generating different versions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ú® Version 1:\n",
            "----------------------------------------\n",
            "Explain technically howhow computer works  How to avoid the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the 'bug'\n",
            "of the 'bug' of the 'bug' of the 'bug' of the 'bug' of the\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ú® Version 2:\n",
            "----------------------------------------\n",
            "Explain technically howhow computer works to understand the world\n",
            "by using the most technical technical means possible.  What is\n",
            "the best computer program you have ever used?  The computer\n",
            "program described below is the best computer program I've ever\n",
            "used.  Click here to download and print the document yourself:\n",
            "http://www.cisco.com/documents/CiscoDocument.pdf\n",
            "----------------------------------------\n",
            "\n",
            "‚ú® Version 3:\n",
            "----------------------------------------\n",
            "Explain technically howhow computer works, or explain something\n",
            "about why it works, or why its operation has a cost.  In fact, it\n",
            "does not matter what computer you use. The user can choose the\n",
            "one to perform the operation.  The user uses your program as long\n",
            "as a computer is running, and they can run it as many times as\n",
            "they want.  The user, on the other hand, can choose the computer\n",
            "to run only in case of an emergency.  This is how most users\n",
            "choose to run a computer as they wish, or they choose to run it\n",
            "when they know the answer should be no. It is not as if they\n",
            "could choose a computer that would run them as they wished.  When\n",
            "you first see this information in print, you may wonder why we\n",
            "had never seen any such system in place before.  In fact, we\n",
            "think of it as our second choice. In fact, the system we have to\n",
            "create for ourselves is just that: a computer program.  With no\n",
            "prior knowledge of what you need to do, you will never have to\n",
            "run your own computer program.  You can never run it without some\n",
            "kind of certification.  A computer program is simply a piece of\n",
            "software that helps you to write code for a\n",
            "----------------------------------------\n",
            "\n",
            "Which version do you like best?\n",
            "You can combine ideas from different versions!\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Enter your topic (or 'quit'): quit\n"
          ]
        }
      ],
      "source": [
        "# CELL 6: Creative Writing Assistant\n",
        "def creative_writing_assistant():\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úçÔ∏è  CREATIVE WRITING ASSISTANT\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nChoose a writing style:\")\n",
        "    print(\"1. Story/Tale\")\n",
        "    print(\"2. Technical/Article\")\n",
        "    print(\"3. Poetic/Creative\")\n",
        "    print(\"4. Business/Professional\")\n",
        "    print(\"5. Custom style\")\n",
        "\n",
        "    style = input(\"\\nChoose style (1-5): \").strip()\n",
        "\n",
        "    styles = {\n",
        "        '1': \"Write a compelling story about\",\n",
        "        '2': \"Explain technically how\",\n",
        "        '3': \"Describe poetically\",\n",
        "        '4': \"Analyze the business implications of\",\n",
        "        '5': \"\"\n",
        "    }\n",
        "\n",
        "    prefix = styles.get(style, \"\")\n",
        "\n",
        "    if style == '5':\n",
        "        prefix = input(\"Enter your custom prefix: \").strip()\n",
        "        if prefix and not prefix.endswith(' '):\n",
        "            prefix += ' '\n",
        "\n",
        "    print(f\"\\nStyle: {prefix if prefix else 'No prefix'}\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        topic = input(\"\\nüìù Enter your topic (or 'quit'): \").strip()\n",
        "\n",
        "        if topic.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        full_prompt = f\"{prefix}{topic}\" if prefix else topic\n",
        "\n",
        "        print(f\"\\nüîÆ Generating: '{full_prompt}'\")\n",
        "\n",
        "        # Generate multiple options\n",
        "        print(\"\\nüí° Generating different versions...\")\n",
        "\n",
        "        for i in range(3):\n",
        "            result = text_generator(\n",
        "                full_prompt,\n",
        "                max_length=150,\n",
        "                temperature=0.7 + (i * 0.1),  # Vary creativity\n",
        "                num_return_sequences=1,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            print(f\"\\n‚ú® Version {i+1}:\")\n",
        "            print(\"-\"*40)\n",
        "            wrapped = textwrap.fill(result[0]['generated_text'], width=65)\n",
        "            print(wrapped)\n",
        "            print(\"-\"*40)\n",
        "\n",
        "        print(\"\\nWhich version do you like best?\")\n",
        "        print(\"You can combine ideas from different versions!\")\n",
        "\n",
        "# Run creative assistant\n",
        "creative_writing_assistant()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g166c2hFrc84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "34ebfc52-a6cd-4a32-f5a7-c3c419af59a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ SAVING YOUR WORK\n",
            "==================================================\n",
            "\n",
            "Generate and save some text:\n",
            "Enter prompt for saving: love is so good that u think\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating from: 'love is so good that u think'\n",
            "\n",
            "üìÑ Generated Text:\n",
            "============================================================\n",
            "love is so good that u think it could be the one you would be\n",
            "more happy with. I mean u need to make sure you can say it right.\n",
            "I'd say, no, it would be fine.  But, you're the one who just said\n",
            "you're going to say it.  Yeah, I'm the one who is saying it.\n",
            "Really?  What do you mean?  Yeah, I mean if you are going to say\n",
            "it right, I'm going to say it right.  You're that guy.  Right?\n",
            "Yeah, for sure.  I mean, you would say it right, but I don't\n",
            "know, I'd like to say it right.  Well, you're the one who said\n",
            "you're going to say it right.  Exactly.  And your friend called\n",
            "you a racist, right?  Yeah, he called me a racist a lot longer\n",
            "than I would have liked to know.  Yeah.  What's he calling you?\n",
            "He called me a racist, I mean, when I was little, he was like,\n",
            "you know what? I was like, okay, he calls me a racist, but why\n",
            "don't you call me racist\n",
            "============================================================\n",
            "\n",
            "‚úÖ Saved to: gpt2_output_1766592706.txt\n",
            "\n",
            "‚¨áÔ∏è  Downloading file...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1873590989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Generate and save some text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0msaved_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_generated_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüéâ PROJECT COMPLETE!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1873590989.py\u001b[0m in \u001b[0;36msave_generated_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Download to your computer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚¨áÔ∏è  Downloading file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "# CELL 7: Save Generated Content\n",
        "print(\"üíæ SAVING YOUR WORK\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def save_generated_text():\n",
        "    print(\"\\nGenerate and save some text:\")\n",
        "    prompt = input(\"Enter prompt for saving: \").strip()\n",
        "\n",
        "    if prompt:\n",
        "        print(f\"\\nGenerating from: '{prompt}'\")\n",
        "        result = text_generator(prompt, max_length=200, num_return_sequences=1)\n",
        "        generated = result[0]['generated_text']\n",
        "\n",
        "        # Display\n",
        "        print(\"\\nüìÑ Generated Text:\")\n",
        "        print(\"=\"*60)\n",
        "        print(textwrap.fill(generated, width=65))\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save to file\n",
        "        filename = f\"gpt2_output_{int(time.time())}.txt\"\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(f\"Prompt: {prompt}\\n\")\n",
        "            f.write(\"-\"*50 + \"\\n\")\n",
        "            f.write(generated)\n",
        "\n",
        "        print(f\"\\n‚úÖ Saved to: {filename}\")\n",
        "\n",
        "        # Download to your computer\n",
        "        print(\"\\n‚¨áÔ∏è  Downloading file...\")\n",
        "        files.download(filename)\n",
        "\n",
        "        return generated\n",
        "    else:\n",
        "        print(\"No prompt provided.\")\n",
        "        return None\n",
        "\n",
        "# Generate and save some text\n",
        "saved_text = save_generated_text()\n",
        "\n",
        "print(\"\\nüéâ PROJECT COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nWhat you've accomplished:\")\n",
        "print(\"1. ‚úÖ Tested pre-trained GPT-2\")\n",
        "print(\"2. ‚úÖ Used interactive text generator\")\n",
        "print(\"3. ‚úÖ Prepared custom dataset\")\n",
        "print(\"4. ‚úÖ Optional: Fine-tuned on your data\")\n",
        "print(\"5. ‚úÖ Used creative writing assistant\")\n",
        "print(\"6. ‚úÖ Saved generated content\")\n",
        "print(\"\\nYou can run any cell again to generate more text!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmvltaJylxxLqMdjUTj15z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}